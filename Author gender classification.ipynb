{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Copy of Baseline Gender Classifier.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "cp5ch0h9uPcv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b4fb9a67-4bec-4935-d73d-b967f6a72309"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0yciragsd44",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy import array\n",
        "import codecs\n",
        "from os import listdir\n",
        "import codecs\n",
        "import re\n",
        "import string\n",
        "import pickle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.models import Model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Input\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM, Bidirectional\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "from keras.layers.merge import concatenate\n",
        "from keras.utils.vis_utils import plot_model\n",
        "language = 'ENGLISH'\n",
        "path_prefix = ''\n",
        "if language == 'ENGLISH':\n",
        "    path_prefix = 'English files/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-fk94Lqsd4-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cleanUp(inp):\n",
        "    tokens = inp.split()\n",
        "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "    tokens = [re_punc.sub(' ', w) for w in tokens]\n",
        "    tokens = [word for word in tokens if word.isalpha()]\n",
        "    tokens = [word for word in tokens if len(word) > 1]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "def load_docs(given_gender):\n",
        "  docs = []\n",
        "  file = open('/content/drive/My Drive/Paper/classtrain.txt', 'r', encoding='utf-8')\n",
        "  text = file.read().split('\\n')\n",
        "  for each in text:\n",
        "    gender = each.split('\\t')[0]\n",
        "    comment = each.split('\\t')[1]\n",
        "    if gender == 'male' and given_gender == 'male':\n",
        "      docs.append(cleanUp(comment))\n",
        "    elif gender == 'female' and given_gender == 'female':\n",
        "      docs.append(cleanUp(comment))\n",
        "  file.close()\n",
        "  return docs\n",
        "\n",
        "formal_male_data = None\n",
        "formal_female_data = None\n",
        "\n",
        "formal_male_data = load_docs('male')\n",
        "formal_female_data = load_docs('female')\n",
        "print('ENGLISH docs loaded')\n",
        "\n",
        "all_data = formal_male_data + formal_female_data\n",
        "all_labels = [1] * len(formal_male_data) + [0] * len(formal_female_data)\n",
        "print(len(all_data))\n",
        "print(len(formal_male_data), len(formal_female_data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLZ_BwIdwRSS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "22b6e295-37d7-4f5e-eebf-8a1cc48ff9da"
      },
      "source": [
        "formal_male_data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['do visit this place',\n",
              " 'great low cost company',\n",
              " 'came here saturday for the ufc fights',\n",
              " 'everything was hot and crispy',\n",
              " 'that enough to check the place out',\n",
              " 'the managers seemed to be running nice place',\n",
              " 'it very nice to feel remembered',\n",
              " 'she choose the grande',\n",
              " 'at the joint we were treated as customers',\n",
              " 'nice selection of bbq and sides']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJCRA4aksd6X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file = None\n",
        "if language == 'PERSIAN':\n",
        "    file = open('Persian Word Embedding/cc.fa.300.vec', 'r', encoding='utf-8', errors='ignore')\n",
        "else:\n",
        "    file = open('English Word Embedding/cc.en.300.vec', 'r', encoding='utf-8', errors='ignore')\n",
        "\n",
        "vocab_and_vectors = {}\n",
        "\n",
        "for line in file:\n",
        "    values = line.split()\n",
        "    word = values[0].encode('utf-8').decode('utf-8')\n",
        "    vector = np.asarray(values[1:], dtype='float32')\n",
        "    vocab_and_vectors[word] = vector\n",
        "\n",
        "print(len(vocab_and_vectors))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13WvoZY_sd6Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "dd78fb47-a515-4920-c956-25e3697862a4"
      },
      "source": [
        "# import these modules \n",
        "from nltk.stem import PorterStemmer \n",
        "from nltk.tokenize import word_tokenize \n",
        "   \n",
        "ps = PorterStemmer() \n",
        "  \n",
        "# choose some words to be stemmed \n",
        "words = [\"program\", \"programs\", \"programer\", \"programing\", \"programers\"] \n",
        "  \n",
        "for w in words: \n",
        "    print(w, \" : \", ps.stem(w)) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "program  :  program\n",
            "programs  :  program\n",
            "programer  :  program\n",
            "programing  :  program\n",
            "programers  :  program\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0e9rLkmsd6h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "fb9840ce-dc0a-428b-cdee-a8d5c7fb09cb"
      },
      "source": [
        "features = 300\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(all_data)\n",
        "with open('/content/drive/My Drive/Paper/English files/GenderTokenizer.pickle', 'wb') as handle:\n",
        "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "    print(\"Tokenizer saved.\")\n",
        "word_index = tokenizer.word_index\n",
        "max_length = max([len(sent) for sent in all_data])\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "X = tokenizer.texts_to_sequences(all_data)\n",
        "X = pad_sequences(X, padding='post')\n",
        "y = all_labels\n",
        "print(all_data[1])\n",
        "print(X[1])\n",
        "print(y[1])\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)\n",
        "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5, shuffle=True)\n",
        "print(len(X_train), len(X_val), len(X_test))\n",
        "print(vocab_size, max_length)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokenizer saved.\n",
            "['nice', 'selection', 'of', 'bbq', 'and', 'sides']\n",
            "[ 78 234   5 424   2 733   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0]\n",
            "1\n",
            "2062289 257787 257786\n",
            "20071 96\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "ID_Rw92qsd6p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "86efaf3c-8886-40bc-a725-1fcc6d1c2446"
      },
      "source": [
        "def define_model(vocab_size, length):\n",
        "    # channel 1\n",
        "    inputs1 = Input(shape=(length,))\n",
        "    embedding1 = Embedding(vocab_size, 100)(inputs1)\n",
        "    conv1 = Conv1D(filters=32, kernel_size=4, activation='relu')(embedding1)\n",
        "    drop1 = Dropout(0.5)(conv1)\n",
        "    pool1 = MaxPooling1D(pool_size=2)(drop1)\n",
        "    LSTM1 = LSTM(256, return_sequences=True, recurrent_dropout=0.2)(pool1)\n",
        "    flat1 = Flatten()(LSTM1)\n",
        "    # channel 2\n",
        "    inputs2 = Input(shape=(length,))\n",
        "    embedding2 = Embedding(vocab_size, 100)(inputs2)\n",
        "    conv2 = Conv1D(filters=32, kernel_size=6, activation='relu')(embedding2)\n",
        "    drop2 = Dropout(0.5)(conv2)\n",
        "    pool2 = MaxPooling1D(pool_size=2)(drop2)\n",
        "    LSTM2 = LSTM(256, return_sequences=True, recurrent_dropout=0.2)(pool2)\n",
        "    flat2 = Flatten()(LSTM2)\n",
        "    # channel 3\n",
        "    inputs3 = Input(shape=(length,))\n",
        "    embedding3 = Embedding(vocab_size, 100)(inputs3)\n",
        "    conv3 = Conv1D(filters=32, kernel_size=8, activation='relu')(embedding3)\n",
        "    drop3 = Dropout(0.5)(conv3)\n",
        "    pool3 = MaxPooling1D(pool_size=2)(drop3)\n",
        "    LSTM3 = LSTM(256, return_sequences=True, recurrent_dropout=0.2)(pool3)\n",
        "    flat3 = Flatten()(LSTM3)\n",
        "    # merge\n",
        "    merged = concatenate([flat1, flat2, flat3])\n",
        "    # interpretation\n",
        "    dense1 = Dense(10, activation='relu' )(merged)\n",
        "    outputs = Dense(1, activation='sigmoid' )(dense1)\n",
        "    model = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs)\n",
        "    model. compile(loss='binary_crossentropy' , optimizer='adam' , metrics=['accuracy'])\n",
        "    model.summary()\n",
        "    \n",
        "    return model \n",
        "\n",
        "model = define_model(vocab_size, max_length)\n",
        "checkpoint = ModelCheckpoint(\"/content/drive/My Drive/Paper/English files/model.h5\", monitor='val_accuracy', save_best_only=True, mode='max')\n",
        "model.fit([X_train, X_train, X_train], y_train, epochs=20, validation_data = ([X_val, X_val, X_val], y_val), batch_size=512, callbacks=[checkpoint])\n",
        "model.save('/content/drive/My Drive/Paper/English files/modelMain.h5' )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 96)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            (None, 96)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_3 (InputLayer)            (None, 96)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, 96, 100)      2007100     input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_2 (Embedding)         (None, 96, 100)      2007100     input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_3 (Embedding)         (None, 96, 100)      2007100     input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 93, 32)       12832       embedding_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 91, 32)       19232       embedding_2[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 89, 32)       25632       embedding_3[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 93, 32)       0           conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 91, 32)       0           conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 89, 32)       0           conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1D)  (None, 46, 32)       0           dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_2 (MaxPooling1D)  (None, 45, 32)       0           dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_3 (MaxPooling1D)  (None, 44, 32)       0           dropout_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 46, 256)      295936      max_pooling1d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "lstm_2 (LSTM)                   (None, 45, 256)      295936      max_pooling1d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "lstm_3 (LSTM)                   (None, 44, 256)      295936      max_pooling1d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 11776)        0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "flatten_2 (Flatten)             (None, 11520)        0           lstm_2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "flatten_3 (Flatten)             (None, 11264)        0           lstm_3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 34560)        0           flatten_1[0][0]                  \n",
            "                                                                 flatten_2[0][0]                  \n",
            "                                                                 flatten_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 10)           345610      concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 1)            11          dense_1[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 7,312,425\n",
            "Trainable params: 7,312,425\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 2062289 samples, validate on 257787 samples\n",
            "Epoch 1/20\n",
            "2062289/2062289 [==============================] - 1102s 535us/step - loss: 0.4556 - accuracy: 0.7805 - val_loss: 0.4357 - val_accuracy: 0.7946\n",
            "Epoch 2/20\n",
            "2062289/2062289 [==============================] - 1115s 541us/step - loss: 0.4202 - accuracy: 0.8027 - val_loss: 0.4260 - val_accuracy: 0.8006\n",
            "Epoch 3/20\n",
            "2062289/2062289 [==============================] - 1114s 540us/step - loss: 0.3995 - accuracy: 0.8141 - val_loss: 0.4215 - val_accuracy: 0.8010\n",
            "Epoch 4/20\n",
            "2062289/2062289 [==============================] - 1117s 542us/step - loss: 0.3812 - accuracy: 0.8247 - val_loss: 0.4225 - val_accuracy: 0.8012\n",
            "Epoch 5/20\n",
            "2062289/2062289 [==============================] - 1116s 541us/step - loss: 0.3643 - accuracy: 0.8344 - val_loss: 0.4255 - val_accuracy: 0.8000\n",
            "Epoch 6/20\n",
            "2062289/2062289 [==============================] - 1092s 529us/step - loss: 0.3489 - accuracy: 0.8428 - val_loss: 0.4285 - val_accuracy: 0.7988\n",
            "Epoch 7/20\n",
            "2062289/2062289 [==============================] - 1126s 546us/step - loss: 0.3331 - accuracy: 0.8511 - val_loss: 0.4357 - val_accuracy: 0.7958\n",
            "Epoch 8/20\n",
            "2062289/2062289 [==============================] - 1125s 545us/step - loss: 0.3190 - accuracy: 0.8583 - val_loss: 0.4416 - val_accuracy: 0.7951\n",
            "Epoch 9/20\n",
            "2062289/2062289 [==============================] - 1116s 541us/step - loss: 0.3066 - accuracy: 0.8645 - val_loss: 0.4480 - val_accuracy: 0.7931\n",
            "Epoch 10/20\n",
            "2062289/2062289 [==============================] - 1104s 535us/step - loss: 0.2946 - accuracy: 0.8702 - val_loss: 0.4596 - val_accuracy: 0.7911\n",
            "Epoch 11/20\n",
            "2062289/2062289 [==============================] - 1095s 531us/step - loss: 0.2848 - accuracy: 0.8748 - val_loss: 0.4617 - val_accuracy: 0.7911\n",
            "Epoch 12/20\n",
            "2062289/2062289 [==============================] - 1099s 533us/step - loss: 0.2753 - accuracy: 0.8794 - val_loss: 0.4665 - val_accuracy: 0.7906\n",
            "Epoch 13/20\n",
            "2062289/2062289 [==============================] - 1085s 526us/step - loss: 0.2668 - accuracy: 0.8833 - val_loss: 0.4773 - val_accuracy: 0.7879\n",
            "Epoch 14/20\n",
            "2062289/2062289 [==============================] - 1067s 518us/step - loss: 0.2595 - accuracy: 0.8869 - val_loss: 0.4811 - val_accuracy: 0.7880\n",
            "Epoch 15/20\n",
            "2062289/2062289 [==============================] - 1070s 519us/step - loss: 0.2528 - accuracy: 0.8901 - val_loss: 0.4874 - val_accuracy: 0.7870\n",
            "Epoch 16/20\n",
            "2062289/2062289 [==============================] - 1073s 520us/step - loss: 0.2468 - accuracy: 0.8926 - val_loss: 0.4984 - val_accuracy: 0.7876\n",
            "Epoch 17/20\n",
            "2062289/2062289 [==============================] - 1082s 525us/step - loss: 0.2414 - accuracy: 0.8949 - val_loss: 0.5011 - val_accuracy: 0.7856\n",
            "Epoch 18/20\n",
            "2062289/2062289 [==============================] - 1078s 523us/step - loss: 0.2363 - accuracy: 0.8975 - val_loss: 0.5101 - val_accuracy: 0.7864\n",
            "Epoch 19/20\n",
            "2062289/2062289 [==============================] - 1074s 521us/step - loss: 0.2313 - accuracy: 0.8998 - val_loss: 0.5176 - val_accuracy: 0.7850\n",
            "Epoch 20/20\n",
            "2062289/2062289 [==============================] - 1087s 527us/step - loss: 0.2274 - accuracy: 0.9016 - val_loss: 0.5187 - val_accuracy: 0.7841\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "TcqrLdvCsd6r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        },
        "outputId": "c53d4853-a31f-4767-e434-891d18f346d9"
      },
      "source": [
        "model = load_model('/content/drive/My Drive/Paper/English files/modelMain.h5' )\n",
        "_, acc = model.evaluate([X_train, X_train, X_train], y_train)\n",
        "print('Train Accuracy: %.2f' % (acc*100))\n",
        "d, acc = model.evaluate([X_test, X_test, X_test], y_test)\n",
        "print('Test Accuracy: %.2f' % (acc*100), d)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2062289/2062289 [==============================] - 1817s 881us/step\n",
            "Train Accuracy: 92.53\n",
            "257786/257786 [==============================] - 224s 869us/step\n",
            "Test Accuracy: 78.20 0.5225570126839063\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}